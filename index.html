<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tongtong Liu</title>
  
  <meta name="author" content="Tongtong Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tongtong (Frank) Liu</name>
              </p>
              <p>I am a senior undergraduate student at Wake Forest University, pursuing a double major in Computer Science and Mathematical Business. 
		 I am also an undergraduate researcher advised by <a href="https://alqahtas.sites.wfu.edu/">Dr. Sarra Alqahtani </a> and <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-mqiao">Dr. Mu Qiao </a>, where my major research interests lies in robustness of Reinforcement Learning, explainable Reinforcement Learning (XRL), Machine Learning, and AI Foundational Model.
              </p>
              <p>
                I have also interned in various technology companies like ByteDance (TikTok), IBM, DataMimo, and Mesoor AI. I have expereince in Software Deveopler Engineering, Machine Learning Engineering, and Research Scientists. I co-founded <a href="www.thewakers.org">The Wakers </a> and also participated in lots of different projects around ML.
	      </p>
	      <p style="text-align:center">
                <a href="mailto:liut18@wfu.edu">Email</a> &nbsp/&nbsp
                <a href="data_ltt/Frank_Resume_CS.docx">Resume</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/frank-liu-974b161a3/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/frank47ltt">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="img_ltt/headshot_ltt2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="img_ltt/headshot_ltt2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in Data Mining, Machine Learning, and Reinforcement Learning. Specifically, I focuses on Foundational AI Model, the robustness and explainability in reinforcement learning agents.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="img_ltt/Attack_pic.png" alt="Attack" width="170" height="170">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
	      <!-- <a href="https://waymo.com/research/block-nerf/"> -->
                <papertitle>On Weaponizing Actions in Multi-Agent Reinforcement Learning: Theoretical and Empirical Study on Security and Robustness</papertitle>
              <!-- </a> -->
              <br>
		    <strong> Tongtong Liu </strong> , Joe McCalmon, Cameron Lischke, Md Asifur Rahman, Talal Halabi, Sarra Alqahtani
              <br>
              <em>UAI (in review)</em>, 2022  
              <br>			
              <p></p>
              <p>
		 Cooperative Multi-Agent Reinforcement Learning (c-MARL) enables a team of agents to determine the global optimal policy that maximizes the sum of their accumulated rewards. In this paper, we investigate the robustness of c-MARL to a novel adversarial threat, where we target and weaponize one agent, termed the compromised agent, to create natural observations that are adversarial for its team. The goal is to lure the compromised agent to follow an adversarial policy that pushes activations of its cooperative agents‚Äô policy networks off distribution. 
	      </p>
            </td>
          </tr>
	  
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="img_ltt/ICAART.png" alt="platoon" width="170" height="170">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.scitepress.org/Papers/2021/101921/101921.pdf">
                <papertitle>Robustness-driven Exploration with Probabilistic Metric Temporal Logic</papertitle>
              </a>
              <br>
		    Xiaotian Liu, Pengyi Shi, <strong> Tongtong Liu </strong>, Sarra Alqahtani, Paul Pauca, Miles Silman
              <br>
              <em>ICAART</em>, 2021
              <p>The ability to perform autonomous exploration is essential for unmanned aerial vehicles (UAV) operating in unknown environments where it is difficult to describe the environment beforehand. Algorithms for autonomous exploration often focus on optimizing time and full coverage in a greedy fashion that collect irrelevant data and wastes time navigating areas with no important information. In this paper, we aim to improve the efficiency of exploration by maximizing the probability of detecting valuable information</p>
            </td>
          </tr>    
	      
	  
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="img_ltt/platoon_fig.png" alt="platoon" width="170" height="170">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9288356">
                <papertitle>Multi-Agent Reinforcement Learning for Cooperative Adaptive Cruise Control</papertitle>
              </a>
              <br>
		    Joe McCalmon, Ashley Peake, Benjamin Raiford, <strong> Tongtong Liu </strong>, Sarra Alqahtani
              <br>
              <em>ICTAI</em>, 2020
              <p>A growing trend in the field of autonomous vehicles is the use of platooning. The design of control algorithms for platoons is challenging considering that coordination among vehicles is obtained through diverse communication channels. In this paper, we propose a multi-agent reinforcement learning approach for autonomous vehicles which communicate in a platoon formation.</p>
            </td>
          </tr>
										
					
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website source code is adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
